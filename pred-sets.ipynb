{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04d06ca-dbe8-4fe8-af6f-294a92c1a932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch is running on cuda\n",
      "Torch Graph Models are running on cuda\n",
      "Torch Graph Models are running on cuda\n",
      "Torch Graph Models are running on cuda\n",
      "Torch Graph Models are running on cuda\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as TVDatasets\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data as GraphData \n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv, APPNP, SAGEConv\n",
    "from torch_geometric.nn.models.label_prop import LabelPropagation\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Torch is running on {device}\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from gnn_cp.data.data_manager import GraphDataManager\n",
    "from gnn_cp.models.graph_models import GCN, GAT, APPNPNet, SAGE\n",
    "from gnn_cp.models.model_manager import GraphModelManager\n",
    "from gnn_cp.data.utils import make_dataset_instances\n",
    "import gnn_cp.cp.transformations as cp_t\n",
    "import gnn_cp.cp.graph_transformations as cp_gt\n",
    "from gnn_cp.cp.graph_cp import GraphCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b13cd7-e8dc-41e5-a310-3be3e17e270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_dir = \"configs/config.yaml\"\n",
    "results_dir = \"results\"\n",
    "figures_dir = \"reports/figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c421608c-34f4-4b29-889e-42f69a233117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "specify the dataset index:\n",
      "0: cora_ml\n",
      "1: cora_ml_largest_component\n",
      "2: cora_full\n",
      "3: pubmed\n",
      "4: citeseer\n",
      "5: coauthor_cs\n",
      "6: coauthor_physics\n",
      "7: amazon_computers\n",
      "8: amazon_photo\n",
      " 0\n",
      "specify the model index:\n",
      "0: GCN\n",
      "1: GAT\n",
      "2: SAGE\n",
      "3: MLP\n",
      "4: APPNPNet\n",
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = cora_ml\n",
      "Dataset Loaded Successfully!\n",
      "Following labeled splits:\n",
      "class 0: train=20, val=20\n",
      "class 1: train=20, val=20\n",
      "class 2: train=20, val=20\n",
      "class 3: train=20, val=20\n",
      "class 4: train=20, val=20\n",
      "class 5: train=20, val=20\n",
      "class 6: train=20, val=20\n",
      "====================================\n",
      "Loading Models\n",
      "Loading Models GCN\n",
      "Accuracy: 0.8231307550644568 +- 0.009074238748131642\n",
      "acc=0.8231307550644568 +- 0.009074238748131642\n"
     ]
    }
   ],
   "source": [
    "# loading the baseline settings\n",
    "with open(config_file_dir, 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "general_dataset_config = config.get(\"baseline\", {}).get(\"general_dataset_config\", {})\n",
    "\n",
    "\n",
    "assert os.path.isdir(results_dir), \"The results path does not exist!\"\n",
    "\n",
    "models_cache_dir = os.path.join(results_dir, \"models\")\n",
    "assert os.path.isdir(models_cache_dir), \"Directory to trained models is not found! Maybe first tun the make_baselines.py file\"\n",
    "data_dir = os.path.join(results_dir, \"datasets\")\n",
    "assert os.path.isdir(data_dir), \"Directory to Data Files is not found!\"\n",
    "splits_dir = os.path.join(results_dir, \"splits\")\n",
    "assert os.path.isdir(splits_dir), \"Directory to Data Splits is not found!\"\n",
    "\n",
    "splits_config = config.get(\"baseline\", {}).get(\"general_dataset_config\", {})\n",
    "\n",
    "dataset_names = list(config.get(\"baseline\", {}).get(\"datasets\", {}).keys())\n",
    "models_config = config.get(\"baseline\", {}).get(\"models\", {})\n",
    "model_classes = list(models_config.keys())\n",
    "\n",
    "# Making a directory to store results for CPs\n",
    "cp_results_dir = os.path.join(results_dir, \"cp_results\")\n",
    "if not os.path.isdir(cp_results_dir):\n",
    "    os.mkdir(cp_results_dir)\n",
    "\n",
    "\n",
    "# region\n",
    "# Making dataset-split and model instances\n",
    "dataset_str_list = '\\n'.join([f'{i}: {dataset_name}' for i, dataset_name in enumerate(dataset_names)])\n",
    "dataset_name_idx = int(input(f\"specify the dataset index:\\n{dataset_str_list}\\n\"))\n",
    "dataset_key = dataset_names[int(dataset_name_idx)]\n",
    "\n",
    "model_str_list = '\\n'.join([f'{i}: {model_name}' for i, model_name in enumerate(model_classes)])\n",
    "model_class_idx = int(input(f\"specify the model index:\\n{model_str_list}\\n\"))\n",
    "model_class_name = model_classes[model_class_idx]\n",
    "\n",
    "dataset_manager = GraphDataManager(data_dir, splits_dir)\n",
    "dataset = dataset_manager.get_dataset_from_key(dataset_key).data\n",
    "\n",
    "print(f\"dataset = {dataset_key}\")\n",
    "instances = make_dataset_instances(data_dir, splits_dir, splits_config, models_cache_dir, dataset_key, model_class_name, models_config)\n",
    "\n",
    "instances_accuracy = [instance[\"accuracy\"] for instance in instances]\n",
    "print(f\"acc={np.mean(instances_accuracy)} +- {np.std(instances_accuracy)}\")\n",
    "best_model_accuracy = np.max(instances_accuracy)\n",
    "\n",
    "instances_logits = [\n",
    "    instance[\"model\"].predict(dataset) for instance in instances\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9364cb-c11e-4f92-96ba-a49efba4ffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['idx_to_attr', 'attr_indices', 'attr_shape', 'idx_to_node', 'adj_shape', 'adj_indptr', 'adj_data', 'labels', 'attr_data', 'adj_indices', 'attr_indptr', 'idx_to_class', 'attr_text']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2995"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_class = dataset_manager.get_dataset_from_key(dataset_key)\n",
    "raw_data_path = os.path.join(dataset_class.raw_dir, dataset_class.raw_file_names)\n",
    "raw_data = np.load(raw_data_path, allow_pickle=True)\n",
    "\n",
    "print(list(raw_data))\n",
    "raw_data[\"attr_text\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a976ebd1-e710-4081-a966-191a3eb4cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_idx = 0\n",
    "instance = instances[instance_idx]\n",
    "train_idx, val_idx, test_idx = instance[\"train_idx\"], instance[\"val_idx\"], instance[\"test_idx\"]\n",
    "model = instance[\"model\"]\n",
    "accuracy = instance[\"accuracy\"]\n",
    "logits = instances_logits[instance_idx]\n",
    "true_mask = (F.one_hot(dataset.y) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ca6562-7f48-4127-986a-3f45d6029c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_fraction = 0.5\n",
    "\n",
    "lambda_vals = np.arange(0, 1.51, 0.05).round(3)\n",
    "coverage_values = np.arange(start=accuracy.round(2), stop=1.0, step=0.005)\n",
    "fixed_neigh_coef = 0.55\n",
    "selected_coverage = coverage_values[len(coverage_values)//2]\n",
    "\n",
    "\n",
    "def singleton_hit(pred_set, true_mask):\n",
    "    one_sized_pred = (pred_set.sum(axis=1) == 1)\n",
    "    result = pred_set[true_mask][one_sized_pred].sum().item() / pred_set.shape[0]\n",
    "    return result\n",
    "\n",
    "singleton_hit_metric = lambda pred_set, true_mask: singleton_hit(pred_set, true_mask)\n",
    "set_size_metric = lambda pred_set, true_mask: GraphCP.average_set_size(pred_set)\n",
    "coverage_metric = lambda pred_set, true_mask: GraphCP.coverage(pred_set, true_mask)\n",
    "argmax_accuracy = lambda pred_set, true_mask: GraphCP.argmax_accuracy(pred_set, true_mask)\n",
    "\n",
    "metrics_dict = {\n",
    "    \"empi_coverage\": coverage_metric,\n",
    "    \"average_set_size\": set_size_metric,\n",
    "    \"singleton_hit\": singleton_hit_metric,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c456dc-76e3-4540-91d9-f4a8d95f1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_reg = 0\n",
    "penalty = 0.5\n",
    "lambda_val = 0.7\n",
    "label_mask = F.one_hot(dataset.y).bool()\n",
    "calib_idx, eval_idx, calib_mask, eval_mask = dataset_manager.train_test_split(test_idx, true_mask, training_fraction=calib_fraction)\n",
    "\n",
    "baseline_scores = cp_t.APSTransformation(softmax=True).pipe_transform(logits)\n",
    "base_cp = GraphCP([], coverage_guarantee=selected_coverage)\n",
    "base_cp.calibrate_from_scores(baseline_scores[calib_idx], label_mask[calib_idx])\n",
    "\n",
    "# baseline\n",
    "baseline_pred_set = base_cp.predict_from_scores(baseline_scores)\n",
    "\n",
    "# regular\n",
    "regular_scores = cp_t.RegularizerPenalty(k_reg=k_reg, penalty=penalty).pipe_transform(baseline_scores)\n",
    "cp = GraphCP([], coverage_guarantee=selected_coverage)\n",
    "\n",
    "cp.calibrate_from_scores(regular_scores[calib_idx], label_mask[calib_idx])\n",
    "reg_pred_set = cp.predict_from_scores(regular_scores)\n",
    "\n",
    "# mixing\n",
    "mixing_scores = cp_gt.VertexMPTransformation(neigh_coef=lambda_val, edge_index=dataset.edge_index, n_vertices=dataset.x.shape[0]).pipe_transform(baseline_scores)\n",
    "cp = GraphCP([], coverage_guarantee=selected_coverage)\n",
    "\n",
    "cp.calibrate_from_scores(mixing_scores[calib_idx], label_mask[calib_idx])\n",
    "mix_pred_set = cp.predict_from_scores(mixing_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cec73e9-751b-4980-9dec-1e7d2b2aacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_indices = torch.where(mix_pred_set.sum(dim=-1) == 2)[0].tolist()\n",
    "mix_indices = [idx for idx in mix_indices if idx in eval_idx]\n",
    "\n",
    "reg_indices = torch.where(reg_pred_set.sum(dim=-1) == 2)[0].tolist()\n",
    "reg_indices = [idx for idx in reg_indices if idx in eval_idx]\n",
    "\n",
    "indices = []\n",
    "for idx in mix_indices:\n",
    "    if idx in reg_indices:\n",
    "        mix_preds = mix_pred_set[idx].nonzero().squeeze().tolist()\n",
    "        reg_preds = reg_pred_set[idx].nonzero().squeeze().tolist()\n",
    "        if mix_preds != reg_preds:\n",
    "            indices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07353c8a-4bec-42b1-8b57-a4c17fe5505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('case_study.txt', 'w') as file:\n",
    "    for idx in indices:\n",
    "        # mixing\n",
    "        mix_preds = mix_pred_set[idx].nonzero().squeeze().tolist()\n",
    "        mix_preds = [raw_data['idx_to_class'].tolist()[cls] for cls in mix_preds]\n",
    "        \n",
    "        # regular\n",
    "        reg_preds = reg_pred_set[idx].nonzero().squeeze().tolist()\n",
    "        reg_preds = [raw_data['idx_to_class'].tolist()[cls] for cls in reg_preds]\n",
    "        \n",
    "        output = f\"Node {idx}\\nMixing predictions: ({mix_preds})\\nRegular predictions: ({reg_preds})\\nAbstract: {raw_data['attr_text'][idx]}\\n\\n\"\n",
    "        file.write(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conformal",
   "language": "python",
   "name": "conformal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
